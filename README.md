# RI-TRM Sudoku

**Rule-Initialized Tiny Recursive Models** for Sudoku solving - a complete implementation demonstrating efficient learning with explicit knowledge.

## ğŸ¯ Status: Fully Implemented & Tested

âœ… **163/163 tests passing**
âœ… **~5,300 lines of code**
âœ… **7M parameters (1000Ã— smaller than LLMs)**
âœ… **Ready to use!**

## ğŸš€ Quick Start

### Try the Demo

```bash
# Quick automated demo (30 seconds)
python demo_auto.py

# Full interactive walkthrough (5 minutes)
python demo.py
```

### Run Training

```bash
# Install dependencies
pip install -r requirements.txt

# Train on 100 tasks (~2 minutes)
python experiments/train_minimal.py

# Run all tests
pytest tests/ -v
```

## ğŸ’¡ Key Features

- **Zero-shot verification** - Rules work without training (K_R)
- **Hebbian learning** - Path memory with LTP/LTD (K_P)
- **Tiny network** - 7M parameters vs 7B+ in LLMs
- **No ground truth** - Trains on violations, not solutions
- **Interpretable** - Full reasoning traces
- **Efficient** - Trains on ~1,000 tasks, not billions of tokens

## ğŸ“Š Architecture

```
Layer 1 (K_F): Factual Knowledge     [Not needed for Sudoku]
Layer 2 (K_R): Structural Rules      âœ… Zero-shot verification
Layer 3 (K_P): Path Memory          âœ… Hebbian learning
```

**Network**: 2-layer transformer, 512 hidden dim, 8 heads, RMSNorm, RoPE, SwiGLU

## ğŸ“ Project Structure

```
â”œâ”€â”€ src/              Core implementation
â”‚   â”œâ”€â”€ rules.py          Zero-shot verification (K_R)
â”‚   â”œâ”€â”€ dataset.py        Rule-based generation
â”‚   â”œâ”€â”€ path_memory.py    Hebbian learning (K_P)
â”‚   â”œâ”€â”€ network.py        7M parameter network
â”‚   â””â”€â”€ solver.py         Recursive refinement
â”œâ”€â”€ tests/            163 tests, all passing
â”œâ”€â”€ experiments/      Training scripts
â”œâ”€â”€ docs/             Research paper & implementation plan
â”œâ”€â”€ demo.py           Interactive demo
â””â”€â”€ demo_auto.py      Automated demo
```

## ğŸ“ RI-TRM Principles

âœ“ **Explicit knowledge > Learned** (when possible)
âœ“ **Tasks > Tokens** (train on ~1K, not billions)
âœ“ **Interpretability > Black box**
âœ“ **Efficiency > Scale**

## ğŸ“– Documentation

- **Full README**: [README_COMPLETE.md](README_COMPLETE.md)
- **Research Paper**: [docs/RI-TRM.md](docs/RI-TRM.md)
- **Implementation Plan**: [docs/Sudoku.md](docs/Sudoku.md)

## ğŸ”¬ Example Usage

```python
from src.rules import SudokuRuleGraph
from src.network import create_sudoku_network
from src.path_memory import HebbianPathMemory
from src.solver import RecursiveSolver

# Initialize components
rule_graph = SudokuRuleGraph()
network = create_sudoku_network()  # 7M params
path_memory = HebbianPathMemory()

# Create solver
solver = RecursiveSolver(network, rule_graph, path_memory)

# Solve with interpretable trace
result = solver.solve(grid, return_trace=True)
print(f"Solved: {result.success}")
print(f"Steps: {result.num_steps}")
for step in result.trace:
    print(step)
```

## ğŸ“ˆ Training Results

From `experiments/train_minimal.py`:
- **Loss reduction**: 99.9% (0.0476 â†’ 0.0000)
- **Training time**: ~2 minutes on CPU
- **Model size**: ~50MB
- **Path memory**: Grows automatically during training

## ğŸ§ª Testing

```bash
# All tests
pytest tests/ -v

# Specific suites
pytest tests/test_rules.py -v        # Zero-shot verification
pytest tests/test_network.py -v      # Network architecture
pytest tests/test_solver.py -v       # Recursive refinement

# With coverage
pytest tests/ --cov=src --cov-report=html
```

## ğŸ“Š Comparison

| Model | Parameters | Training Data | Size |
|-------|-----------|---------------|------|
| RI-TRM Sudoku | 7M | ~1,000 tasks | ~50MB |
| GPT-2 Small | 117M | Billions of tokens | ~500MB |
| GPT-3.5 | 175B | Trillions of tokens | ~700GB |

## ğŸ“ Citation

```bibtex
@article{ritrm2025,
  title={Rule-Initialized Tiny Recursive Models},
  author={Jay Noon},
  year={2025},
  note={Under Review}
}
```

## ğŸ“„ License

MIT License

---

**Built following RI-TRM principles: Explicit knowledge when possible, train on tasks not tokens, fully interpretable, efficient.**
